# -*- coding: utf-8 -*-
"""diamond price model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J0ObaXxGHQ4gh04eptGvQ2kzk4sSe7RJ

#Task 1: Define the Problem
"""

# Given the data, for this project you will:
#-  Conduct an EDA which should include:
#   - The use the ML Notebook template.
#   - Visualizations of the data (at least 3).
#   - Visualizations of chosen parameters (if not choosing the whole dataset for intended goal - e.g. regression or classification).
#- Training of several ML models
#   - Conclusive explanation detailing why a particular model was chosen.  e.g. if you chose a decision tree instead of a logistic regression.
#- Two notebooks
#   - One notebook consisting of the EDA and model training
#   - Second notebook used to utilize and make predictions with the saved model.
#- Submit the link to the repo for the project containing the dataset and the notebooks.

"""#Task 2a: Install the Needed Libraries"""

# all libraries installed

"""#Task 2b: Import the Needed Libraries"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import pickle
import joblib

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score
from math import sqrt
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error

"""#Task 3: Load the Data"""

data = pd.read_csv('/content/drive/MyDrive/diamonds.csv')

"""#Task 4: Perform Data Exploratory Analysis (EDA)

##Show the data
"""

data.head()

"""##Get Data Info"""

data.info()

print(data['clarity'].unique().tolist())

print(data['color'].unique().tolist())

print(data['cut'].unique().tolist())

# Set up a 2x2 grid of plots
fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(14, 10), constrained_layout=True)

# Boxplot for price vs color
color_order = ['D', 'E', 'F', 'G', 'H', 'I', 'J']
sns.boxplot(ax=axs[0, 0], data=data, x='color', y='price', order=color_order)
axs[0, 0].set_title('Price vs Color')
axs[0, 0].set_xlabel('Color')
axs[0, 0].set_ylabel('Price ($)')

# Boxplot for price vs clarity
clarity_order = ['IF', 'VVS1', 'VVS2', 'VS1', 'VS2', 'SI1', 'SI2', 'I1']
sns.boxplot(ax=axs[0, 1], data=data, x='clarity', y='price', order=clarity_order)
axs[0, 1].set_title('Price vs Clarity')
axs[0, 1].set_xlabel('Clarity')
axs[0, 1].set_ylabel('Price ($)')

# Scatter plot for price vs carat with regression line
sns.regplot(ax=axs[1, 0], data=data, x='carat', y='price', scatter_kws={'s': 5}, line_kws={'color': 'red'})
axs[1, 0].set_title('Price vs Carat')
axs[1, 0].set_xlabel('Carat')
axs[1, 0].set_ylabel('Price ($)')

# Boxplot for price vs cut
cut_order = ['Ideal', 'Premium', 'Very Good', 'Good', 'Fair']
sns.boxplot(ax=axs[1, 1], data=data, x='cut', y='price', order=cut_order)
axs[1, 1].set_title('Price vs Cut')
axs[1, 1].set_xlabel('Cut')
axs[1, 1].set_ylabel('Price ($)')

# Show the plot
plt.show()

data['volume'] = data['x'] * data['y'] * data['z']
# Create the scatter plot for volume vs carat
plt.figure(figsize=(10, 6))
sns.scatterplot(x='carat', y='volume', data=data)
plt.title('Volume vs Carat')
plt.xlabel('Carat Weight')
plt.ylabel('Volume (mm³)')
plt.show()

outliers = data[data['volume'] > 3500]
print(outliers)

"""##Find Missing Values"""

data.isnull().sum()

"""#Task 5: Perform Data Cleaning"""

x_descriptive_stats = data['x'].describe()
plt.figure(figsize=(10, 6))
plt.hist(data['x'], bins=30, edgecolor='black')
plt.title('Histogram of the x values')
plt.xlabel('Length in mm (x)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
print(x_descriptive_stats)

zero_x_entries = data[data['x'] == 0]
print(zero_x_entries)

data = data[data['x'] != 0]

x_descriptive_stats = data['x'].describe()
plt.figure(figsize=(10, 6))
plt.hist(data['x'], bins=30, edgecolor='black')
plt.title('Histogram of the x values')
plt.xlabel('Length in mm (x)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
print(x_descriptive_stats)

x_descriptive_stats = data['x'].describe()
plt.figure(figsize=(10, 6))
plt.hist(data['x'], bins=30, edgecolor='black', log=True)
plt.title('Histogram of the x values (Logarithmic Scale)')
plt.xlabel('Length in mm (x)')
plt.ylabel('Log(Frequency)')
plt.grid(True)
plt.show()
print(x_descriptive_stats)

y_descriptive_stats = data['y'].describe()
plt.figure(figsize=(10, 6))
plt.hist(data['y'], bins=30, edgecolor='black')
plt.title('Histogram of the y values')
plt.xlabel('Length in mm (y)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
print(y_descriptive_stats)

outlier_y_entries = data[data['y'] > 11]
print(outlier_y_entries)

data = data[data['y'] < 11]

y_descriptive_stats = data['y'].describe()
plt.figure(figsize=(10, 6))
plt.hist(data['y'], bins=30, edgecolor='black', log=True)
plt.title('Histogram of the y values (Logarithmic Scale)')
plt.xlabel('Length in mm (y)')
plt.ylabel('Log(Frequency)')
plt.grid(True)
plt.show()
print(y_descriptive_stats)

z_descriptive_stats = data['z'].describe()
plt.figure(figsize=(10, 6))
plt.hist(data['z'], bins=30, edgecolor='black')
plt.title('Histogram of the z values')
plt.xlabel('Length in mm (z)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
print(z_descriptive_stats)

outlier_z_entries = data[data['z'] > 7]
print(outlier_z_entries)

data = data[data['z'] < 7]

z_descriptive_stats = data['z'].describe()
plt.figure(figsize=(10, 6))
plt.hist(data['z'], bins=30, edgecolor='black', log=True)
plt.title('Histogram of the z values (Logarithmic Scale)')
plt.xlabel('Length in mm (z)')
plt.ylabel('Log(Frequency)')
plt.grid(True)
plt.show()
print(z_descriptive_stats)

data = data[data['z'] != 0]

z_descriptive_stats = data['z'].describe()
plt.figure(figsize=(10, 6))
plt.hist(data['z'], bins=30, edgecolor='black', log=True)
plt.title('Histogram of the z values (Logarithmic Scale)')
plt.xlabel('Length in mm (z)')
plt.ylabel('Log(Frequency)')
plt.grid(True)
plt.show()
print(z_descriptive_stats)

outlier_z_entries = data[data['z'] < 2]
print(outlier_z_entries)

data = data[data['z'] > 2]

z_descriptive_stats = data['z'].describe()
plt.figure(figsize=(10, 6))
plt.hist(data['z'], bins=30, edgecolor='black', log=True)
plt.title('Histogram of the z values (Logarithmic Scale)')
plt.xlabel('Length in mm (z)')
plt.ylabel('Log(Frequency)')
plt.grid(True)
plt.show()
print(z_descriptive_stats)

# Create the scatter plot for volume vs carat
plt.figure(figsize=(10, 6))
sns.scatterplot(x='carat', y='volume', data=data)
plt.title('Volume vs Carat')
plt.xlabel('Carat Weight')
plt.ylabel('Volume (mm³)')
plt.show()

data = data.drop('volume', axis=1)

clarity_mapping = {clarity: rank for rank, clarity in enumerate(clarity_order, start=1)}
print(clarity_mapping)

data['clarity'] = data['clarity'].map(clarity_mapping)
data.head()

cut_mapping = {cut: rank for rank, cut in enumerate(cut_order, start=1)}
print(cut_mapping)

data['cut'] = data['cut'].map(cut_mapping)
data.head()

color_mapping = {color: rank for rank, color in enumerate(color_order, start=1)}
print(color_mapping)

data['color'] = data['color'].map(color_mapping)
data.head()

"""I'm dropping x, y, z, depth, and table as they don't significantly contribute to the accuracy, per earlier testing"""

data = data.drop(['x', 'y', 'z', 'depth', 'table'], axis=1)

data.info()

#scaler = MinMaxScaler()
#data[['carat', 'depth', 'table', 'x', 'y', 'z']] = scaler.fit_transform(data[['carat', 'depth', 'table', 'x', 'y', 'z']])

"""#Task 6: Visualize Cleaned up Dataset

#Task 7: Conceptualize the problem
"""

# estimate price based on carat, depth, table, dimensions, clarity, color, and cut

"""#Task 8: Perform Data-Split"""

print(data.columns)

x = data[['carat', 'cut', 'color', 'clarity']].values
y = data['price'].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 101)

print(f'The shape of the data is: \nx_train: \t{x_train.shape} \nx_test: \t{x_test.shape} \ny_train: \t{y_train.shape} \ny_test: \t{y_test.shape}')

"""#Task 9: Choose the ML Method to follow"""

LinearRegression_model = LinearRegression()
DecisionTree_model = DecisionTreeRegressor()
RandomForest_model = RandomForestRegressor()
XGBRegressor_model = XGBRegressor()

"""#Task 10: Train the ML Model"""

models = [LinearRegression_model, DecisionTree_model, RandomForest_model, XGBRegressor_model]
for model in models:
    model.fit(x_train, y_train)
    print(f'{model} is trained!')

accuracy_LinearRegression = LinearRegression_model.score(x_test, y_test)
accuracy_DecisionTree = DecisionTree_model.score(x_test, y_test)
accuracy_RandomForest = RandomForest_model.score(x_test, y_test)
accuracy_XGBoost = XGBRegressor_model.score(x_test, y_test)

models = {'LinearRegression_model': accuracy_LinearRegression, 'DecisionTree_model': accuracy_DecisionTree, 'RandomForest_model': accuracy_RandomForest,
          'XGBRegressor_model': accuracy_XGBoost}
for model, score in models.items():
    print(f'The accuracy score for the {model} is {round(score*100, 2)}%')

# Number of folds
k = 10

# Setting up the K-Fold cross-validation
kf = KFold(n_splits=k, shuffle=True, random_state=42)

# Perform k-fold cross-validation and compute the mean accuracy for each model
linear_reg_score = cross_val_score(LinearRegression_model, x, y, cv=kf).mean()
decision_tree_score = cross_val_score(DecisionTree_model, x, y, cv=kf).mean()
random_forest_score = cross_val_score(RandomForest_model, x, y, cv=kf).mean()
xgb_score = cross_val_score(XGBRegressor_model, x, y, cv=kf).mean()

# Print the scores
print(f"Linear Regression CV Score: {round(linear_reg_score*100, 2)}")
print(f"Decision Tree CV Score: {round(decision_tree_score*100, 2)}")
print(f"Random Forest CV Score: {round(random_forest_score*100, 2)}")
print(f"XGBRegressor CV Score: {round(xgb_score*100, 2)}")

models = [LinearRegression_model, DecisionTree_model, RandomForest_model, XGBRegressor_model]
for model in models:
    y_pred = model.predict(x_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    print(f"{model.__class__.__name__} RMSE: {round(rmse*100, 2)}")

"""I'm choosing the XGBRegressor model for its high accuracy, robustness as confirmed by K-fold cross-validation, and the lowest Root Mean Squared Error (RMSE)."""

print(data.columns)

feature_data = data.drop('price', axis=1)
print(feature_data.columns)

# Retrieve feature importances
importances = XGBRegressor_model.feature_importances_

# Convert the importances into a more readable format and sort them
feature_importances = pd.Series(importances, index=feature_data.columns).sort_values(ascending=False)

print(feature_importances)

predictions = XGBRegressor_model.predict(x)

plt.figure(figsize=(10, 6))
plt.scatter(x[:, 0], y, color='blue', alpha=0.5, label='Actual Values')

# Plotting the predicted values
plt.scatter(x[:, 0], predictions, color='red', alpha=0.5, label='Predicted Values')

# Adding titles and labels
plt.title('Actual vs Predicted Values')
plt.xlabel('Some Feature')
plt.ylabel('Target Value')
plt.legend()

# Show the plot
plt.show()

"""#Task 11: Test the Model"""

choice = 10000
row_to_predict = data.iloc[choice]
actual_value = row_to_predict['price']
row_to_predict = row_to_predict.drop('price')
row_to_predict = row_to_predict.values.reshape(1, -1)
prediction = XGBRegressor_model.predict(row_to_predict)
print(row_to_predict)
print(f"The predicted value is: {prediction[0]:.2f}")
print(f"The actual value is: {actual_value:.2f}")

joblib.dump(XGBRegressor_model, '/content/drive/MyDrive/diamond_price.joblib')